2025-01-25 07:35:54,384 - INFO - Loaded hyperparameters: [{'GPU_device': 'cuda:1', 'filter': True, 'num_epochs': 500, 'early_stop': 500, 'num_classes': 2, 'd_state': 16, 'd_conv': 2, 'expand': 8, 'scale_factor': 1, 'batch_size': 64}]. 

2025-01-25 07:35:54,384 - INFO - TUAB 

2025-01-25 07:35:54,502 - INFO - GPU: cuda:1
 model name: BrainMamba2_multibranch
2025-01-25 07:35:54,531 - INFO - Loaded TUAB dataset.
2025-01-25 07:35:54,531 - INFO - Training set: Number of batches: 766
2025-01-25 07:35:54,531 - INFO - Training set: Total number of trails: 48994

2025-01-25 07:35:54,531 - INFO - Validation set: Number of batches: 192
2025-01-25 07:35:54,531 - INFO - Validation set: Total number of trails: 12229

2025-01-25 07:35:54,531 - INFO - Testing set: Number of batches: 95
2025-01-25 07:35:54,531 - INFO - Testing set: Total number of trails: 6067

2025-01-25 07:35:55,516 - INFO - Shape for each batch: torch.Size([64, 16, 7680])
2025-01-25 07:35:55,517 - INFO -     # Duration (s): 7680   # of channels: 16

2025-01-25 07:35:56,248 - INFO - Layer input_embedding.branch1.weight: Shape torch.Size([64, 16, 1])
2025-01-25 07:35:56,248 - INFO - Layer input_embedding.branch1.bias: Shape torch.Size([64])
2025-01-25 07:35:56,248 - INFO - Layer input_embedding.branch3.weight: Shape torch.Size([64, 16, 3])
2025-01-25 07:35:56,248 - INFO - Layer input_embedding.branch3.bias: Shape torch.Size([64])
2025-01-25 07:35:56,248 - INFO - Layer input_embedding.branch7.weight: Shape torch.Size([64, 16, 7])
2025-01-25 07:35:56,248 - INFO - Layer input_embedding.branch7.bias: Shape torch.Size([64])
2025-01-25 07:35:56,248 - INFO - Layer input_embedding.fuse.weight: Shape torch.Size([64, 192, 1])
2025-01-25 07:35:56,248 - INFO - Layer input_embedding.fuse.bias: Shape torch.Size([64])
2025-01-25 07:35:56,248 - INFO - Total trainable parameters: 4565144

2025-01-25 07:35:56,249 - INFO - Model Summary: 
 BrainMamba2_multibranch(
  (input_embedding): MultiBranchInputEmbedding(
    (branch1): Conv1d(16, 64, kernel_size=(1,), stride=(1,))
    (branch3): Conv1d(16, 64, kernel_size=(3,), stride=(1,), padding=(1,))
    (branch7): Conv1d(16, 64, kernel_size=(7,), stride=(1,), padding=(3,))
    (fuse): Conv1d(192, 64, kernel_size=(1,), stride=(1,))
  )
  (encoder1): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): SelfSupervisedMambaModel(
      (mamba): Mamba2(
        (in_proj): Linear(in_features=64, out_features=1064, bias=False)
        (conv1d): Conv1d(544, 544, kernel_size=(2,), stride=(1,), padding=(1,), groups=544)
        (act): SiLU()
        (norm): RMSNorm()
        (out_proj): Linear(in_features=512, out_features=64, bias=False)
      )
      (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
  )
  (pool1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (encoder2): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=(1,))
  (pool2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (encoder3): Conv1d(128, 256, kernel_size=(3,), stride=(1,), padding=(1,))
  (pool3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (bottleneck): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): SelfSupervisedMambaModel(
      (mamba): Mamba2(
        (in_proj): Linear(in_features=256, out_features=4160, bias=False)
        (conv1d): Conv1d(2080, 2080, kernel_size=(2,), stride=(1,), padding=(1,), groups=2080)
        (act): SiLU()
        (norm): RMSNorm()
        (out_proj): Linear(in_features=2048, out_features=256, bias=False)
      )
      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (2): Linear(in_features=256, out_features=256, bias=True)
  )
  (decoder3): Conv1d(512, 256, kernel_size=(3,), stride=(1,), padding=(1,))
  (decodeMamba3): SelfSupervisedMambaModel(
    (mamba): Mamba2(
      (in_proj): Linear(in_features=256, out_features=4160, bias=False)
      (conv1d): Conv1d(2080, 2080, kernel_size=(2,), stride=(1,), padding=(1,), groups=2080)
      (act): SiLU()
      (norm): RMSNorm()
      (out_proj): Linear(in_features=2048, out_features=256, bias=False)
    )
    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
  (decoder2): Conv1d(384, 128, kernel_size=(3,), stride=(1,), padding=(1,))
  (decodeMamba2): SelfSupervisedMambaModel(
    (mamba): Mamba2(
      (in_proj): Linear(in_features=128, out_features=2096, bias=False)
      (conv1d): Conv1d(1056, 1056, kernel_size=(2,), stride=(1,), padding=(1,), groups=1056)
      (act): SiLU()
      (norm): RMSNorm()
      (out_proj): Linear(in_features=1024, out_features=128, bias=False)
    )
    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
  )
  (decoder1): Conv1d(192, 64, kernel_size=(3,), stride=(1,), padding=(1,))
  (onput_embedding): Conv1d(64, 16, kernel_size=(1,), stride=(1,))
)

2025-01-25 07:35:56,249 - INFO - Training start.
2025-01-27 05:44:43,919 - INFO -  - Channel 1 MSE: 9.212566820515633e-10
2025-01-27 05:44:43,919 - INFO -  - Channel 2 MSE: 1.6351130172065353e-10
2025-01-27 05:44:43,919 - INFO -  - Channel 3 MSE: 1.5479592607725579e-10
2025-01-27 05:44:43,919 - INFO -  - Channel 4 MSE: 8.169238202571094e-11
2025-01-27 05:44:43,919 - INFO -  - Channel 5 MSE: 9.507908349526417e-11
2025-01-27 05:44:43,919 - INFO -  - Channel 6 MSE: 6.336111080251783e-10
2025-01-27 05:44:43,920 - INFO -  - Channel 7 MSE: 9.601586886676117e-11
2025-01-27 05:44:43,920 - INFO -  - Channel 8 MSE: 8.272964258093651e-11
2025-01-27 05:44:43,920 - INFO -  - Channel 9 MSE: 4.5212722454834875e-10
2025-01-27 05:44:43,920 - INFO -  - Channel 10 MSE: 1.1211673811573064e-09
2025-01-27 05:44:43,920 - INFO -  - Channel 11 MSE: 1.540793603815871e-10
2025-01-27 05:44:43,920 - INFO -  - Channel 12 MSE: 2.815415400814203e-10
2025-01-27 05:44:43,920 - INFO -  - Channel 13 MSE: 1.4484234645006921e-10
2025-01-27 05:44:43,920 - INFO -  - Channel 14 MSE: 7.547741320834689e-10
2025-01-27 05:44:43,920 - INFO -  - Channel 15 MSE: 3.8098066368341676e-10
2025-01-27 05:44:43,920 - INFO -  - Channel 16 MSE: 7.07152836287861e-10
2025-01-27 05:44:43,920 - INFO -  - Average MSE across all channels computed from Original and Reconstructed ERPs: 8.105934218120836e-13
